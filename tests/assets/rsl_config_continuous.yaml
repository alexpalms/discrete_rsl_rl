num_envs: 2

train_cfg:
  runner_class_name: "OnPolicyRunner"
  num_steps_per_env: 24
  seed: 1
  obs_groups: {"policy": ["policy"], "critic": ["policy"]}
  save_interval: 100
  empirical_normalization: null
  runner:
    checkpoint: -1
    experiment_name: test_continuous_PPO_rsl
    load_run: -1
    log_interval: 1
    max_iterations: 10
    record_interval: -1
    resume: False
    resume_path: null
    run_name: ""
  policy:
    activation: "elu"
    actor_hidden_dims: [64, 64]
    critic_hidden_dims: [64, 64]
    init_noise_std: 1.0
    class_name: "ActorCritic"
  algorithm:
    class_name: "PPO"
    clip_param: 0.2
    desired_kl: 0.01
    entropy_coef: 0.01
    gamma: 0.99
    lam: 0.95
    learning_rate: 0.001
    max_grad_norm: 1.0
    num_learning_epochs: 5
    num_mini_batches: 4
    schedule: "adaptive"
    use_clipped_value_loss: True
    value_loss_coef: 1.0
  init_member_classes: {}
